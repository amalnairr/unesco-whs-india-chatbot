{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Web Scraping Code**\n",
    "This code downloads the text description of the world heritage sites in India from Wikipedia using the Wikipedia API, BeautifulSoup and Requests. It does so in two stages:\n",
    "- 1) Extracts links of Individual Sites from https://en.wikipedia.org/wiki/List_of_World_Heritage_Sites_in_India\n",
    "- 2) Extracts the text content from each link into a separate markdown file and stores them into a folder called 'sites'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wikipedia-api --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Extraction of Links from the WHS List page\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "page = 'List_of_World_Heritage_Sites_in_India'\n",
    "# Wikipedia page URL\n",
    "wiki_url = f\"https://en.wikipedia.org/wiki/{page}\"  # Replace with the actual Wikipedia page URL\n",
    "\n",
    "# Fetch the HTML content of the Wikipedia page\n",
    "response = requests.get(wiki_url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "pages = [page]\n",
    "# Find the table with the specified class and iterate through rows to extract links\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "if table:\n",
    "    links = table.select('th[scope=\"row\"] a')\n",
    "    \n",
    "    # Extract and print the href attributes (links)\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if '%27' in href:\n",
    "            href = href.replace('%27', \"'\")\n",
    "        if href.startswith('/wiki/'):\n",
    "            pages.append(href[6:])\n",
    "\n",
    "else:\n",
    "    print(\"Table not found on the Wikipedia page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 pages out of 43 done.\n",
      "2 pages out of 43 done.\n",
      "3 pages out of 43 done.\n",
      "4 pages out of 43 done.\n",
      "5 pages out of 43 done.\n",
      "6 pages out of 43 done.\n",
      "7 pages out of 43 done.\n",
      "8 pages out of 43 done.\n",
      "9 pages out of 43 done.\n",
      "10 pages out of 43 done.\n",
      "11 pages out of 43 done.\n",
      "12 pages out of 43 done.\n",
      "13 pages out of 43 done.\n",
      "14 pages out of 43 done.\n",
      "15 pages out of 43 done.\n",
      "16 pages out of 43 done.\n",
      "17 pages out of 43 done.\n",
      "18 pages out of 43 done.\n",
      "19 pages out of 43 done.\n",
      "20 pages out of 43 done.\n",
      "21 pages out of 43 done.\n",
      "22 pages out of 43 done.\n",
      "23 pages out of 43 done.\n",
      "24 pages out of 43 done.\n",
      "25 pages out of 43 done.\n",
      "26 pages out of 43 done.\n",
      "27 pages out of 43 done.\n",
      "28 pages out of 43 done.\n",
      "29 pages out of 43 done.\n",
      "30 pages out of 43 done.\n",
      "31 pages out of 43 done.\n",
      "32 pages out of 43 done.\n",
      "33 pages out of 43 done.\n",
      "34 pages out of 43 done.\n",
      "35 pages out of 43 done.\n",
      "36 pages out of 43 done.\n",
      "37 pages out of 43 done.\n",
      "38 pages out of 43 done.\n",
      "39 pages out of 43 done.\n",
      "40 pages out of 43 done.\n",
      "41 pages out of 43 done.\n",
      "42 pages out of 43 done.\n",
      "43 pages out of 43 done.\n"
     ]
    }
   ],
   "source": [
    "#Part 2: Extracting the text from the Wikipedia pages of individual World Heritage Sites and saving it as markdown files\n",
    "\n",
    "import wikipediaapi\n",
    "\n",
    "def get_wikipedia_text(page_title):\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('wikipedia_agent_en')  # 'en' for English Wikipedia, change if needed\n",
    "    page = wiki_wiki.page(page_title)\n",
    "\n",
    "    if not page.exists():\n",
    "        return \"Page not found.\"\n",
    "\n",
    "    return page.text\n",
    "\n",
    "def save_as_markdown(text, folder_path, file_name):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "count = 1\n",
    "for page_title in pages:\n",
    "    result = get_wikipedia_text(page_title)\n",
    "    if result != \"Page not found.\":\n",
    "        # Replace 'output.md' with your desired file path and name\n",
    "        save_as_markdown(result, 'sites', page_title + '.md')\n",
    "        print(f\"{count} pages out of {len(pages)} done.\")\n",
    "    else:\n",
    "        print(result)\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
